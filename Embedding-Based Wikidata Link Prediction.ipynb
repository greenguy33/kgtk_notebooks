{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678af3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e57de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_label = \"P106\"\n",
    "label = 'Yval'\n",
    "percent_train_data = .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "110f4681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading claims data for property P106 ...(may take some time depending on chosen property)\n",
      "P106 claims: 8273101\n",
      "Found 6339032 distinct nodes with at least one P106 relationship\n"
     ]
    }
   ],
   "source": [
    "# Read input file and create subject/object map\n",
    "all_lines = []\n",
    "node_map = {}\n",
    "print(\"Reading claims data for property\",property_label,\"...(may take some time depending on chosen property)\")\n",
    "with open(f\"../data/propertiesSplit_final/claims.{property_label}.tsv\", encoding=\"utf-8\") as in_file:\n",
    "    tsv_reader = csv.reader(in_file, delimiter=\"\\t\")\n",
    "    header = next(tsv_reader)\n",
    "    for line in tsv_reader:\n",
    "        all_lines.append(line)\n",
    "        if line[0] in node_map:\n",
    "            node_map[line[0]].append(line[2])\n",
    "        else:\n",
    "            node_map[line[0]] = [line[2]]\n",
    "print(property_label,\"claims:\",len(all_lines))\n",
    "print(\"Found\",len(node_map),\"distinct nodes with at least one\",property_label,\"relationship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f126da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shuffled output file...\n",
      "Shuffled output written to claims.P106.tsv\n"
     ]
    }
   ],
   "source": [
    "# create shuffled output file\n",
    "print(\"Creating shuffled output file...\")\n",
    "random.shuffle(all_lines)\n",
    "with open(f\"../data/propertiesSplit_final/claims.{property_label}.shuffled.tsv\", \"w\", encoding=\"utf-8\", newline='') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter=\"\\t\")\n",
    "    tsv_writer.writerow(header)\n",
    "    for line in all_lines:\n",
    "        tsv_writer.writerow(line)\n",
    "print(\"Shuffled output written to claims.\"+property_label+\".tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acf0f52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the transE file...(this will take some time)\n",
      "Created training data file for property P106 with 5852167 rows\n"
     ]
    }
   ],
   "source": [
    "# create embeddings-based training data\n",
    "if (exists(f\"../data/link_prediction_data/{property_label}.transE.tsv\")):\n",
    "    print(f\"File ../data/link_prediction_data/{property_label}.transE.tsv already exists!\")\n",
    "else:\n",
    "    transEheader = ['Qnode']\n",
    "    for i in range(0,100):\n",
    "        transEheader.append(\"pos\"+str(i))\n",
    "    transEheader.append(\"Yval\")\n",
    "    linecount = 0\n",
    "    print(\"Reading the transE file...(this will take some time)\")\n",
    "    with open(f\"../embeddings/profile_graph_embeddings.transE.tsv\", encoding=\"utf-8\") as in_file, open(f\"../data/link_prediction_data/{property_label}.transE.tsv\", \"w\", encoding=\"utf-8\", newline='') as out_file:\n",
    "        tsv_reader = csv.reader(in_file, delimiter=\" \")\n",
    "        tsv_writer = csv.writer(out_file, delimiter=\"\\t\")\n",
    "        next(tsv_reader)\n",
    "        tsv_writer.writerow(transEheader)\n",
    "        for line in tsv_reader:\n",
    "            if line[0] in node_map:\n",
    "                this_arr = []\n",
    "                for elem in line:\n",
    "                    this_arr.append(elem)\n",
    "                this_arr.append(random.choice(node_map[line[0]]))\n",
    "                tsv_writer.writerow(this_arr)\n",
    "                linecount += 1\n",
    "    print(\"Created training data file for property\",property_label,\"with\",linecount,\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "908ef0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making train/test split for property P106 with 75 % training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: ../data/link_prediction_data/P106.transE.tsv | Columns = 102 / 102 | Rows = 5852167 -> 5852167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 200000\n",
      "Training data size: 150000\n",
      "Test data size: 50000\n"
     ]
    }
   ],
   "source": [
    "# make train/test split\n",
    "print(\"Making train/test split for property\",property_label,\"with\",str(percent_train_data)[2:],\"% training data...\")\n",
    "# limit data to 200,000 training rows\n",
    "all_data = TabularDataset(f\"../data/link_prediction_data/{property_label}.transE.tsv\")[:200000]\n",
    "print(\"Data length:\",int(len(all_data)))\n",
    "train_data = all_data[:int(len(all_data)*percent_train_data)]\n",
    "test_data = all_data[int(len(all_data)*percent_train_data):]\n",
    "print(\"Training data size:\",len(train_data))\n",
    "print(\"Test data size:\",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9b8591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../embeddings/models/agModels.P106.transE\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (150000 samples, 139.53 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"../embeddings/models/agModels.P106.transE\\\"\n",
      "AutoGluon Version:  0.4.0\n",
      "Python Version:     3.8.0\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    150000\n",
      "Train Data Columns: 101\n",
      "Label Column: Yval\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 2568) unique label values:  ['Q1650915', 'Q5716684', 'Q3391743', 'Q1234713', 'Q1800680', 'Q13381376', 'Q47064', 'Q1622272', 'Q189290', 'Q82955']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Updated label_count_threshold from 10 to 7 to avoid cutting too many classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of class variable: \n",
      " count       150000\n",
      "unique        2568\n",
      "top       Q1650915\n",
      "freq         42885\n",
      "Name: Yval, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Some classes in the training set have fewer than 7 examples. AutoGluon will only keep 764 out of 2568 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 7 examples that will be kept for training models: 0.9760866666666667\n",
      "Train Data Class Count: 764\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17975.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 126.75 MB (0.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Qnode']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['Qnode']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 100 | ['pos0', 'pos1', 'pos2', 'pos3', 'pos4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 100 | ['pos0', 'pos1', 'pos2', 'pos3', 'pos4', ...]\n",
      "\t3.3s = Fit runtime\n",
      "\t100 features in original data used to generate 100 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 117.13 MB (0.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.016666666666666666, Train Rows: 143972, Val Rows: 2441\n",
      "\tWARNING: \"NN\" model has been deprecated in v0.4.0 and renamed to \"NN_MXNET\". Starting in v0.5.0, specifying \"NN\" or \"NN_MXNET\" will raise an exception. Consider instead specifying \"NN_TORCH\".\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: KNeighbors ...\n",
      "\t0.6846\t = Validation score   (accuracy)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t12.97s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 23.033 GB, but only 17.683 GB is available...\n",
      "\tNot enough memory to train LightGBM... Skipping this model.\n",
      "Fitting model: ExtraTrees ...\n",
      "\tWarning: Model is expected to require 414.92% of available memory (Estimated before training)...\n",
      "\tNot enough memory to train ExtraTrees... Skipping this model.\n",
      "Fitting model: NeuralNetMXNet ...\n",
      "\tWarning: Exception caused NeuralNetMXNet to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency mxnet. A quick tip is to install via `pip install mxnet --upgrade`, or `pip install mxnet_cu101 --upgrade`\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.6846\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 34.92s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../embeddings/models/agModels.P106.transE\\\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: WeightedEnsemble_L2\n",
      "Model saved to ../embeddings/models/agModels.P106.transE\n"
     ]
    }
   ],
   "source": [
    "# fit autogluon models\n",
    "save_path = f'../embeddings/models/agModels.{property_label}.transE'\n",
    "print(\"Summary of class variable: \\n\", train_data[label].describe())\n",
    "predictor = TabularPredictor(label=label, path=save_path, learner_kwargs={\"label_count_threshold\":10}).fit(train_data, hyperparameters={'NN':{},'GBM':{},'XT':{},'KNN':{}})\n",
    "print(\"Best model:\",predictor.get_model_best())\n",
    "print(f\"Model saved to ../embeddings/models/agModels.{property_label}.transE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df6b7ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions over test data...\n",
      "Overall accuracy: 0.73622\n"
     ]
    }
   ],
   "source": [
    "# uncomment the 2 lines below to load the saved model; if the above cell has just been ran this is not necessary\n",
    "# save_path = f'../embeddings/models/agModels.{property_label}.{orientation}.transE.synchronized.250000'\n",
    "# predictor = TabularPredictor.load(save_path)\n",
    "print(\"Making predictions over test data...\")\n",
    "y_test = test_data[label]\n",
    "test_data_nolab = test_data.drop(columns=[label])\n",
    "y_pred = list(predictor.predict(test_data_nolab))\n",
    "entries = list(test_data[\"Qnode\"])\n",
    "correct = 0\n",
    "total = 0\n",
    "for index, entry in enumerate(entries):\n",
    "    prediction = y_pred[index]\n",
    "    if prediction in node_map[entry]:\n",
    "        correct+=1\n",
    "    total+=1\n",
    "print(\"Overall accuracy:\",(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ccb89dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: ../data/temp/custom_test_P106.tsv | Columns = 102 / 102 | Rows = 2 -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for Benjamin Franklin\n",
      "{'Q1622272': 0.4000000059604645, 'Q10873124': 0.20000000298023224, 'Q170790': 0.20000000298023224, 'Q593644': 0.20000000298023224}\n",
      "Probabilities for Leonardo DiCaprio\n",
      "{'Q2259451': 0.6000000238418579, 'Q2405480': 0.20000000298023224, 'Q2526255': 0.20000000298023224}\n"
     ]
    }
   ],
   "source": [
    "# calculate top-k metric\n",
    "#nodes_to_check = {\"Q34969\":[\"Q37226\",\"Q2259451\"], \"Q38111\":[\"Q37226\",\"Q2259451\"]}\n",
    "node_list = [\"Q34969\",\"Q38111\"]\n",
    "\n",
    "# count = 0\n",
    "# print(\"Calculating requested node probabilities...\")\n",
    "# with open(f\"../data/link_prediction_data/{property_label}.transE.tsv\") as in_file, open(f\"../data/temp/custom_test_{property_label}.tsv\", 'w', encoding=\"utf-8\", newline='') as out_file:\n",
    "#     tsv_reader = csv.reader(in_file, delimiter=\"\\t\")\n",
    "#     tsv_writer = csv.writer(out_file, delimiter=\"\\t\")\n",
    "#     header = next(tsv_reader)\n",
    "#     tsv_writer.writerow(header)\n",
    "#     for line in tsv_reader:\n",
    "#         if line[0] in nodes_to_check:\n",
    "#             tsv_writer.writerow(line)\n",
    "#             count = count + 1\n",
    "#             if count == len(nodes_to_check):\n",
    "#                 break\n",
    "\n",
    "preddata = TabularDataset(f\"../data/temp/custom_test_{property_label}.tsv\").drop(columns=[label])\n",
    "probabilities = predictor.predict_proba(preddata)\n",
    "probabilities_dict = probabilities.to_dict()\n",
    "\n",
    "# for qnode in nodes_to_check:\n",
    "#     for test_node in nodes_to_check[qnode]:\n",
    "#         index = preddata.index[preddata[\"Qnode\"] == qnode].tolist()[0]\n",
    "#         prob = probabilities_dict[test_node][index]\n",
    "#         print(f\"The predicted probability of {qnode} -> {property_label} -> {test_node} is {prob}\")\n",
    "\n",
    "for node in node_list:\n",
    "    index = preddata.index[preddata[\"Qnode\"] == node].tolist()[0]\n",
    "    node_list = {}\n",
    "    for key in probabilities_dict:\n",
    "        if probabilities_dict[key][index] > 0:\n",
    "            node_list[key] = probabilities_dict[key][index]\n",
    "    if node == \"Q34969\":\n",
    "        print(\"Probabilities for Benjamin Franklin\")\n",
    "    elif node == \"Q38111\":\n",
    "        print(\"Probabilities for Leonardo DiCaprio\")\n",
    "    print({k: v for k, v in sorted(node_list.items(), key=lambda item: item[1], reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6f151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
